{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cdfa30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 주가 관련 기사 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "971b8c16",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ssl import SSLError\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from urllib import parse\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import socket\n",
    "import random\n",
    "import argparse\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "f984fd9d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def crawl(query, save_as, begin, end, sort=0, field=1, delay=0.5, timeout=30, page_limit=50):\n",
    "    '''\n",
    "    :param query: 네이버 '뉴스'란에서 검색할 검색어\n",
    "    :param save_as: 검색 결과 저장 경로\n",
    "    :param begin: '기간' -> 검색 기간 시작\n",
    "    :param end: '기간' -> 검색 기간 끝\n",
    "    :param sort: '유형' -> 0(관련도순) 1(최신순) 2(오래된순)\n",
    "    :param field: '영역' -> 0(전체) 1(제목)\n",
    "    :param delay: (옵션) 검색 리퀘스트 간격 (초)\n",
    "    :param timeout: (옵션) 타임아웃 시 기다릴 시간 (초)\n",
    "    :param page_limit: (옵션) 검색 결과에서 몇 페이지까지 갈 것인지 결정\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # prerequisite\n",
    "    df = pd.DataFrame(columns=['link', 'title', 'date', 'article'])\n",
    "\n",
    "    # index settings\n",
    "    # a single pages includes 10 news, starting from page 1 (index 1~10)\n",
    "    current_index = 1\n",
    "    max_index = 2\n",
    "\n",
    "    while (current_index <= max_index) and (1 + current_index // 10 <= page_limit):\n",
    "        url = make_url(query, sort, field, begin, end, current_index)\n",
    "        bsobj = make_bsobj(url, delay, timeout, trial=10)\n",
    "        if bsobj is None:\n",
    "            continue\n",
    "        naver_news_urls = make_naver_news_urls(bsobj)\n",
    "        naver_news_title = get_naver_news_title(bsobj)\n",
    "        try:\n",
    "            img_url = get_article_img_url(bsobj)\n",
    "        except:\n",
    "            img_url = \" \"\n",
    "        naver_news_articles = []\n",
    "        try:\n",
    "            if len(naver_news_urls) != 0:\n",
    "                url = naver_news_urls[0]\n",
    "                news_bsobj = BeautifulSoup(url, 'lxml')\n",
    "\n",
    "                naver_news_article = get_naver_news_article_one(bsobj)\n",
    "                naver_news_articles.append(naver_news_article[0])\n",
    "            else:\n",
    "                naver_news_title = \" \"\n",
    "                naver_news_articles = \" \"\n",
    "                img_url = \" \"\n",
    "\n",
    "    #             date, article, title, newspaper = attributes\n",
    "            df = pd.DataFrame([ x for x in zip(naver_news_title, naver_news_articles, img_url)])\n",
    "            df.columns = ['naver_news_title', 'naver_news_articles', 'naver_news_img_url']\n",
    "            current_index += 10\n",
    "\n",
    "            max_index = get_max_index(bsobj)\n",
    "            return df\n",
    "        except:\n",
    "            naver_news_title = \" \"\n",
    "            naver_news_articles = \" \"\n",
    "            img_url = \" \"\n",
    "            df = pd.DataFrame([ x for x in zip(naver_news_title, naver_news_articles, img_url)])\n",
    "            df.columns = ['naver_news_title', 'naver_news_articles', 'naver_news_img_url']\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "daa08984",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_max_index(bsobj):\n",
    "    paging = bsobj.find_all('a', 'btn_next')\n",
    "    if not paging:\n",
    "        print('(WARNING!) no results found')\n",
    "        return None\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "0db9def4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_naver_news_urls(bsobj):\n",
    "    return [link['href'] for link in bsobj.find_all('a', 'news_tit')]\n",
    "\n",
    "def get_naver_news_title(bsobj):\n",
    "    return [link.text for link in bsobj.find_all('a', 'news_tit')]\n",
    "\n",
    "def get_naver_news_article_one(bsobj):\n",
    "    return [article.text for article in bsobj.find_all('a', 'api_txt_lines dsc_txt_wrap')]\n",
    "\n",
    "def get_article_img_url(bsobj):\n",
    "    return [img['src'] for img in bsobj.find_all('img', 'thumb api_get')]\n",
    "\n",
    "def get_naver_news_article(url, news_bsobj):\n",
    "    try:\n",
    "        html = urlopen(url).read()\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        return text\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "f98e7aa6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_url(query, sort, field, begin, end, page):\n",
    "    url = \"https://search.naver.com/search.naver?&where=news&query=\" + parse.quote(query)\n",
    "    url += \"&sort=%i\" % sort\n",
    "    url += \"&field=%i\" % field\n",
    "    url += \"&ds=\" + begin + \"&de=\" + end\n",
    "    url += \"&nso=so:r,p:\"\n",
    "    url += \"from\" + begin.replace(\".\", \"\") + \"to\" + end.replace(\".\", \"\")\n",
    "    url += \"&start=\" + str(page)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "260ef478",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_bsobj(url, delay=0.5, timeout=30, trial=10):\n",
    "    ua = UserAgent(verify_ssl=False)\n",
    "    count = 0\n",
    "\n",
    "    while count < trial:\n",
    "        try:\n",
    "            time.sleep(delay + random.random())\n",
    "            html = urlopen(Request(url=url, headers={'User-Agent': ua.random}), timeout=timeout)\n",
    "            bsobj = BeautifulSoup(html, 'lxml')\n",
    "            return bsobj\n",
    "        except (URLError, SSLError, socket.timeout) as e:\n",
    "            print('(Error)', e)\n",
    "            print('reloading...')\n",
    "            count += 1\n",
    "            time.sleep(timeout)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "7732b3de",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%A0%9C%EC%A3%BC%EB%8F%84%EA%B4%80%EA%B4%91&sort=0&photo=0&field=1&pd=3&ds=2015.01.01&de=2015.01.30&cluster_rank=13&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from20150101to20150130,a:t&start=11'\n",
    "delay=0.5\n",
    "timeout=30\n",
    "bsobj = make_bsobj(url, delay, timeout, trial=10)\n",
    "max_index = get_max_index(bsobj)\n",
    "if max_index is None:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a0ba3b5c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    # Argument configuration\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--query', type=str, required=True, help='query to search on NAVER')\n",
    "    parser.add_argument('--begin', type=str, required=True, help='crawling begin point (%%Y.%%m.%%d format)')\n",
    "    parser.add_argument('--end', type=str, required=True, help='crawling end point (%%Y.%%m.%%d format)')\n",
    "    parser.add_argument('--sort', type=int, default=0, help='search result sorting: 0(relevant), 1(newest), 2(oldest)')\n",
    "    parser.add_argument('--field', type=int, default=1, help='search field: 0(all), 1(title)')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "062f913c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ran_num(n):\n",
    "    ls = []\n",
    "    num = random.randint(1, n)\n",
    "    while n in ls :\n",
    "        num = random.randint(1, n)\n",
    "        ls.append(num)\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "9deb0f98",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('news_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "deea7563",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_e = crawl('엔씨소프트', f'./test.xlsx', f'2021.10.12', f'2021.06.06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "cccf7020",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naver_news_title</th>\n",
       "      <th>naver_news_articles</th>\n",
       "      <th>naver_news_img_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>엔씨소프트 ‘리니지W’, 사전 캐릭터명 선점 열기 ‘후끈’</td>\n",
       "      <td>신규 월드 추가하고 6차 이벤트 진행 엔씨소프트(대표 김택진)의 신작 멀티플랫폼 M...</td>\n",
       "      <td>https://search.pstatic.net/common/?src=https%3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   naver_news_title  \\\n",
       "0  엔씨소프트 ‘리니지W’, 사전 캐릭터명 선점 열기 ‘후끈’   \n",
       "\n",
       "                                 naver_news_articles  \\\n",
       "0  신규 월드 추가하고 6차 이벤트 진행 엔씨소프트(대표 김택진)의 신작 멀티플랫폼 M...   \n",
       "\n",
       "                                  naver_news_img_url  \n",
       "0  https://search.pstatic.net/common/?src=https%3...  "
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "ee6226c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['start_date'] = list(map(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').strftime('%Y.%m.%d'), df['start_date']))\n",
    "df['end_date'] = list(map(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').strftime('%Y.%m.%d'), df['end_date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "bbc741c1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(WARNING!) no results found\n",
      "(WARNING!) no results found\n",
      "(WARNING!) no results found\n",
      "(WARNING!) no results found\n",
      "(WARNING!) no results found\n",
      "(WARNING!) no results found\n",
      "(WARNING!) no results found\n",
      "(WARNING!) no results found\n",
      "(WARNING!) no results found\n",
      "(WARNING!) no results found\n",
      "(WARNING!) no results found\n"
     ]
    }
   ],
   "source": [
    "news_title = []\n",
    "news_article = []\n",
    "news_img_url = []\n",
    "for index, row in df.iterrows():\n",
    "    df_e = crawl(row['stock'], f'./test.xlsx', row['start_date'], row['end_date'])\n",
    "    news_title.append(df_e.iat[0,0])\n",
    "    news_article.append(df_e.iat[0,1])\n",
    "    news_img_url.append(df_e.iat[0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "16140a35",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem_data = pd.read_csv('./problem_data.csv', index_col=0)\n",
    "problem_data['news_title'] = news_title\n",
    "problem_data['news_article'] = news_article\n",
    "problem_data['news_img_url'] = news_img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "87070867",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drop_index = problem_data[problem_data['news_title'] == ' '].index\n",
    "problem_data.drop(drop_index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "6f7e7e3a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem_data = problem_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "problem_data = problem_data.drop(columns=\"start_date\")\n",
    "problem_data.to_csv('result.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tendencys = ['공격투자형', '적극투자형', '위험중립형', '안정추구형', '안정형']\n",
    "tendency_explanations = ['시장평균 수익률을 훨씬 넘어서는 수준의 투자수익을 추구하며 이을 위해 자산가치의 변동에 따른 손실위험을 적극 수용, 투자자금 대부분을 주식, 주식형 펀드 또는 파생상품 등의 위험자산에 투자할 의향이 있는 유형', '투자원금의 보전보다는 위험을 감내하더라도 높은 수준의 투자수익 실현을 추구하고 투자 자금 상당부분을 주식, 주식형 펀드 또는 파생상품 등의 위험자산에 투자할 의향이 있는 유형', '투자자는 그에 상응하는 투자위험이 있음을 충분히 인식하고 있으며, 예ㆍ적금보다 높은 수익을 기대할 수 있다면 일정수준의 위험을 감수 할 수 있는 유형', '투자원금의 손실위험은 최소화 하고, 이자소득이나 배당소득 수준의 안정적인 투자를 목표로 함. 다만, 수익을 위해 단기적인 손실을 수용할 수 있으며, 예ㆍ적금보다 높은 수익을 위해 자산 중 일부를 변동성 높은 상품에 투자할 의향이 있는 유형', '예금 또는 적금 수준의 수익율을 기대하며, 투자원금에 손실이 발생하는 것을 원하지 않음']\n",
    "tendencys_solution = ['']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "b007caca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'위험중립형'"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tendency.loc[2]['tendency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f345bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tendency = pd.DataFrame()\n",
    "tendency['tendency'] = tendencys\n",
    "tendency['tendency_explanation'] = tendency_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "8d39bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tendency.to_csv('tendency_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}